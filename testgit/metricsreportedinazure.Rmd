---
title: "metricsreportedinazure"
author: "Shaheen"
date: "Wednesday, January 13, 2016"
---

### Reproduce metrics reported in Azure ML for multiclass classification using R  
*Please note the confusion matric reported in R is transpose of confusion matrix reported in Azure!  *  
(Python and Azure use the same convention however)  
```{r}
#create a confusion matrix as dataframe
cm_df = data.frame(col1=c('TP','FP'),col2=c('FN','TN')) 
row.names(cm_df)=c('Actual_A','Actual_B')
colnames(cm_df)=c('Pred_A','pred_B')
cat('In Azure (and python) the convention is as follows \n')
cm_df  # as in Azure
```



```{r}

#create a confusion matrix as dataframe
cm_df = data.frame(col1=c(10,15,0),col2=c(8,50,3),col3=c(2,5,7)) 

#cm_df = data.frame(col1=c(32,0,0),col2=c(0,14,0),col3=c(0,7,22))

#cm_df = data.frame(col1=c(850,139,20),col2=c(615228,346)

#cm_df = data.frame(col1=c(850,139),col2=c(0,14,0),col3=c(0,7,22))

row.names(cm_df)=c('Actual_A','Actual_B','Actual_C')
colnames(cm_df)=c('Pred_A','pred_B','Pred_C')
cm_df

#convert data frame to matrix
cm_matrix = data.matrix(cm_df,rownames.force=NA)
cm_matrix


#Recall
recall_mcc = function(i){cm_matrix[i,i]/rowSums(cm_matrix)[i]}
recall_mcc(1)  #recall for label 1
recall_mcc(2)
recall_mcc(3)

#Precision 
precision_mcc = function(i){cm_matrix[i]/colSums(cm_matrix)[i]}
precision_mcc(1)  #precision for label 1
precision_mcc(2)
precision_mcc(3)

#Accuracy # this is OverallAccuracy
accuracy = sum(diag(cm_matrix))/sum(cm_matrix)
accuracy   #0.67
#The accuracy usually works very well when the classes are balanced in multi-class learning

#F1 score
F1score_mcc = function(i){2*precision_mcc(i)*recall_mcc(i)/(precision_mcc(i)+recall_mcc(i))}
F1score_mcc(1)  #F1 score for label 1
F1score_mcc(2)
F1score_mcc(3)

#In summary, macro-averaged F1 treats all classes equally; but micro-averaged F1 considers the size of each class. 

#Macro averaged
macro_av_precision = (precision_mcc(1)+precision_mcc(2)+precision_mcc(3) )/3
macro_av_precision

macro_av_recall = (recall_mcc(1)+recall_mcc(2)+recall_mcc(3) )/3
macro_av_recall

macro_av_F1score = (F1score_mcc(1)+F1score_mcc(2)+F1score_mcc(3))/3
macro_av_F1score   #0.5970455

#Micro averaged
#function to get confusion matrix for each label
cm_labels = function(i){matrix( c(cm_matrix[i,i],colSums(cm_matrix)[i]-cm_matrix[i,i] ,rowSums(cm_matrix)[i]-cm_matrix[i,i] ,  sum(cm_matrix)-rowSums(cm_matrix)[i]-colSums(cm_matrix)[i]+ cm_matrix[i,i] ), nrow=2,ncol=2)}
cm_1 = cm_labels(1);cm_1
cm_2 = cm_labels(2);cm_2
cm_3 = cm_labels(3);cm_3



#add these 3 confusion matrices
cm_123 = cm_1 + cm_2 + cm_3
cm_123
cm_matrix = cm_123

##For micro F score
micro_av_F1score = F1score_mcc(1)
micro_av_F1score

#For micro precsion
micro_av_precision = precision_mcc(1)
micro_av_precision

#For micro recall
micro_av_recall = recall_mcc(1)
micro_av_recall

accuracy = sum(diag(cm_matrix))/sum(cm_matrix)
accuracy  #This is average accuracy!!



```

